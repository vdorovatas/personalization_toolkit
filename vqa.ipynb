{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/shared/home/SSO3984/Pekit-github-dev/github page/personalization_toolkit/vqa.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bi-0f97a3877d3bdf20b/shared/home/SSO3984/Pekit-github-dev/github%20page/personalization_toolkit/vqa.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtempfile\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bi-0f97a3877d3bdf20b/shared/home/SSO3984/Pekit-github-dev/github%20page/personalization_toolkit/vqa.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcollections\u001b[39;00m \u001b[39mimport\u001b[39;00m defaultdict\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bi-0f97a3877d3bdf20b/shared/home/SSO3984/Pekit-github-dev/github%20page/personalization_toolkit/vqa.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m setup_args, Azure_evaluate, get_cat_folders_from_features, draw_bounding_box, mask_to_normalized_bbox, get_query, get_objects_features, apply_mask_dino, get_mask_for_gt\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bi-0f97a3877d3bdf20b/shared/home/SSO3984/Pekit-github-dev/github%20page/personalization_toolkit/vqa.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m \u001b[39mimport\u001b[39;00m Image\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bi-0f97a3877d3bdf20b/shared/home/SSO3984/Pekit-github-dev/github%20page/personalization_toolkit/vqa.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n",
      "File \u001b[0;32m~/Pekit-github-dev/github page/personalization_toolkit/utils.py:17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mopenai\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain_openai\u001b[39;00m \u001b[39mimport\u001b[39;00m ChatOpenAI\n\u001b[1;32m     18\u001b[0m py_logger \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mgetLogger(\u001b[39m__name__\u001b[39m)\n\u001b[1;32m     19\u001b[0m py_logger\u001b[39m.\u001b[39msetLevel(logging\u001b[39m.\u001b[39mDEBUG)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/langchain_openai/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain_openai\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchat_models\u001b[39;00m \u001b[39mimport\u001b[39;00m AzureChatOpenAI, ChatOpenAI\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain_openai\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39membeddings\u001b[39;00m \u001b[39mimport\u001b[39;00m AzureOpenAIEmbeddings, OpenAIEmbeddings\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain_openai\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mllms\u001b[39;00m \u001b[39mimport\u001b[39;00m AzureOpenAI, OpenAI\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/langchain_openai/chat_models/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain_openai\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchat_models\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mazure\u001b[39;00m \u001b[39mimport\u001b[39;00m AzureChatOpenAI\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain_openai\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchat_models\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m ChatOpenAI\n\u001b[1;32m      4\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mChatOpenAI\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mAzureChatOpenAI\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/langchain_openai/chat_models/azure.py:11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Any, Callable, Optional, TypedDict, TypeVar, Union\n\u001b[1;32m     10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mopenai\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain_core\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlanguage_models\u001b[39;00m \u001b[39mimport\u001b[39;00m LanguageModelInput\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain_core\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlanguage_models\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchat_models\u001b[39;00m \u001b[39mimport\u001b[39;00m LangSmithParams\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain_core\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmessages\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseMessage\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1412\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/langchain_core/language_models/__init__.py:112\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr_name)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(attr_name: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mobject\u001b[39m:\n\u001b[1;32m    111\u001b[0m     module_name \u001b[39m=\u001b[39m _dynamic_imports\u001b[39m.\u001b[39mget(attr_name)\n\u001b[0;32m--> 112\u001b[0m     result \u001b[39m=\u001b[39m import_attr(attr_name, module_name, __spec__\u001b[39m.\u001b[39mparent)\n\u001b[1;32m    113\u001b[0m     \u001b[39mglobals\u001b[39m()[attr_name] \u001b[39m=\u001b[39m result\n\u001b[1;32m    114\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/langchain_core/_import_utils.py:36\u001b[0m, in \u001b[0;36mimport_attr\u001b[0;34m(attr_name, module_name, package)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 36\u001b[0m         module \u001b[39m=\u001b[39m import_module(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mmodule_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, package\u001b[39m=\u001b[39mpackage)\n\u001b[1;32m     37\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mModuleNotFoundError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m     38\u001b[0m         msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmodule \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpackage\u001b[39m!r}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mmodule_name\u001b[39m!r}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m not found (\u001b[39m\u001b[39m{\u001b[39;00merr\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/zeroshot_github/lib/python3.12/importlib/__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m     89\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 90\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/langchain_core/language_models/base.py:44\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain_core\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moutputs\u001b[39;00m \u001b[39mimport\u001b[39;00m LLMResult\n\u001b[1;32m     43\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 44\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m GPT2TokenizerFast  \u001b[39m# type: ignore[import-not-found]\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     _HAS_TRANSFORMERS \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/__init__.py:958\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n\u001b[1;32m    956\u001b[0m _import_structure \u001b[39m=\u001b[39m {k: \u001b[39mset\u001b[39m(v) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m _import_structure\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m--> 958\u001b[0m import_structure \u001b[39m=\u001b[39m define_import_structure(Path(\u001b[39m__file__\u001b[39m)\u001b[39m.\u001b[39mparent \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmodels\u001b[39m\u001b[39m\"\u001b[39m, prefix\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmodels\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    959\u001b[0m import_structure[\u001b[39mfrozenset\u001b[39m({})]\u001b[39m.\u001b[39mupdate(_import_structure)\n\u001b[1;32m    961\u001b[0m sys\u001b[39m.\u001b[39mmodules[\u001b[39m__name__\u001b[39m] \u001b[39m=\u001b[39m _LazyModule(\n\u001b[1;32m    962\u001b[0m     \u001b[39m__name__\u001b[39m,\n\u001b[1;32m    963\u001b[0m     \u001b[39mglobals\u001b[39m()[\u001b[39m\"\u001b[39m\u001b[39m__file__\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    966\u001b[0m     extra_objects\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39m__version__\u001b[39m\u001b[39m\"\u001b[39m: __version__},\n\u001b[1;32m    967\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/utils/import_utils.py:2867\u001b[0m, in \u001b[0;36mdefine_import_structure\u001b[0;34m(module_path, prefix)\u001b[0m\n\u001b[1;32m   2843\u001b[0m \u001b[39m@lru_cache\u001b[39m\n\u001b[1;32m   2844\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefine_import_structure\u001b[39m(module_path: \u001b[39mstr\u001b[39m, prefix: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m IMPORT_STRUCTURE_T:\n\u001b[1;32m   2845\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2846\u001b[0m \u001b[39m    This method takes a module_path as input and creates an import structure digestible by a _LazyModule.\u001b[39;00m\n\u001b[1;32m   2847\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2865\u001b[0m \u001b[39m    If `prefix` is not None, it will add that prefix to all keys in the returned dict.\u001b[39;00m\n\u001b[1;32m   2866\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2867\u001b[0m     import_structure \u001b[39m=\u001b[39m create_import_structure_from_path(module_path)\n\u001b[1;32m   2868\u001b[0m     spread_dict \u001b[39m=\u001b[39m spread_import_structure(import_structure)\n\u001b[1;32m   2870\u001b[0m     \u001b[39mif\u001b[39;00m prefix \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/utils/import_utils.py:2580\u001b[0m, in \u001b[0;36mcreate_import_structure_from_path\u001b[0;34m(module_path)\u001b[0m\n\u001b[1;32m   2578\u001b[0m \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mlistdir(module_path):\n\u001b[1;32m   2579\u001b[0m     \u001b[39mif\u001b[39;00m f \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__pycache__\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misdir(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(module_path, f)):\n\u001b[0;32m-> 2580\u001b[0m         import_structure[f] \u001b[39m=\u001b[39m create_import_structure_from_path(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(module_path, f))\n\u001b[1;32m   2582\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misdir(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(directory, f)):\n\u001b[1;32m   2583\u001b[0m         adjacent_modules\u001b[39m.\u001b[39mappend(f)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/utils/import_utils.py:2604\u001b[0m, in \u001b[0;36mcreate_import_structure_from_path\u001b[0;34m(module_path)\u001b[0m\n\u001b[1;32m   2601\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m module_name\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39m.py\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m   2602\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m-> 2604\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(directory, module_name), encoding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m   2605\u001b[0m     file_content \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mread()\n\u001b[1;32m   2607\u001b[0m \u001b[39m# Remove the .py suffix\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Simplified VQA validation using dataloader.\n",
    "\n",
    "This script uses the improved dataloader which builds the dataset\n",
    "directly from the VQA JSON file, ensuring only images with QA pairs are loaded.\n",
    "\"\"\"\n",
    "\n",
    "print('started')\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import tempfile\n",
    "from collections import defaultdict\n",
    "from utils import setup_args, Azure_evaluate, get_cat_folders_from_features, draw_bounding_box, mask_to_normalized_bbox, get_query, get_objects_features, apply_mask_dino, get_mask_for_gt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import glob\n",
    "import copy\n",
    "from loader import get_dataloader\n",
    "from model import Extractor, Personalized_InternVL\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "print('import finished')\n",
    "\n",
    "# === Setup ===\n",
    "args = setup_args()\n",
    "args.dataset = 'this-is-my'\n",
    "args.task = 'vqa'\n",
    "args.features_folder = '/shared/home/SSO3984/Pekit-github-dev/features_folder/'\n",
    "args.multi_concept = False\n",
    "args.device_ids = [7]\n",
    "args.vlm_model = \"OpenGVLab/InternVL3-14B\"\n",
    "args.n_training_views = 5\n",
    "#args.variation = \"augment\"\n",
    "#args.n_augment = 10\n",
    "args.grounding_sam = True\n",
    "args.split = 'test'  # Set to validation/test split\n",
    "args.batch_size = 1  # Process one at a time for VQA\n",
    "args.shuffle = False  # Don't shuffle for consistent results\n",
    "args.num_workers = 0  # Simpler for debugging\n",
    "# Override JSON path if needed\n",
    "args.json_path = \"/fsx/ad/vlm/github_datasets_test/This-is-My-Img/Multi-concept/this-is-my-visual-qa-multi-concept.json\"\n",
    "\n",
    "if args.variation == 'normal':\n",
    "    args.n_augment = 1\n",
    "\n",
    "colors = ['blue', 'red', 'green', 'purple', 'orange', 'pink', 'gray', 'yellow']\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s %(message)s',\n",
    "    datefmt='%H:%M'\n",
    ")\n",
    "\n",
    "# === Load dataloader ===\n",
    "logging.info(\"Loading dataloader...\")\n",
    "dataloader, my_objects, context_pool = get_dataloader(args)\n",
    "logging.info(f\"Dataloader loaded with {len(dataloader.dataset)} VQA samples\")\n",
    "logging.info(f\"Found {len(my_objects)} object categories\")\n",
    "\n",
    "# === Initialize models ===\n",
    "logging.info(\"Loading models...\")\n",
    "mask_extractor = Extractor(args)\n",
    "azure = Azure_evaluate ()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if 'InternVL' in args.vlm_model:\n",
    "    vlm_model = Personalized_InternVL(args)\n",
    "else:\n",
    "    sys.exit(f\"{args.vlm_model} is not supported\")\n",
    "\n",
    "# === Load object features ===\n",
    "logging.info(\"Loading object features...\")\n",
    "# Get training views from the feature folder\n",
    "cat_folders = get_cat_folders_from_features(args)\n",
    "feature_objects = [os.path.basename(folder) for folder in cat_folders]\n",
    "\n",
    "all_obj_files, all_obj_features = get_objects_features(args, my_objects)\n",
    "logging.info(f\"Features loaded for {len(feature_objects)} objects\")\n",
    "\n",
    "# Debug: Print feature objects\n",
    "logging.info(f\"Feature objects found: {feature_objects}\")\n",
    "for obj in feature_objects:\n",
    "    logging.info(f\"  - '{obj}'\")\n",
    "\n",
    "# Create a mapping from object name to features\n",
    "object_to_features = {}\n",
    "for idx, obj_name in enumerate(feature_objects):\n",
    "    object_to_features[obj_name] = all_obj_features[idx].unsqueeze(0)\n",
    "\n",
    "# Create name mapping for multi-concept lookups\n",
    "# Maps \"Casey\" -> \"Casey-man\", \"Alex\" -> \"Alex-woman\", etc.\n",
    "name_to_full_object = {}\n",
    "for obj in feature_objects:\n",
    "    # Extract base name (before hyphen)\n",
    "    if '-' in obj:\n",
    "        base_name = obj.split('-')[0]\n",
    "        # Handle possessive forms: \"Caseys\" -> \"Casey\"\n",
    "        if base_name.endswith('s') and len(base_name) > 1:\n",
    "            base_name_without_s = base_name[:-1]\n",
    "            if base_name_without_s not in name_to_full_object:\n",
    "                name_to_full_object[base_name_without_s] = obj\n",
    "        if base_name not in name_to_full_object:\n",
    "            name_to_full_object[base_name] = obj\n",
    "    # Also add the full object name without suffix for object lookups\n",
    "    # e.g., \"Caseys boosted board-skateboard\" -> also map \"Caseys boosted board\"\n",
    "    if '-' in obj:\n",
    "        base_obj = '-'.join(obj.split('-')[:-1])  # Remove suffix after last hyphen\n",
    "        if base_obj not in name_to_full_object:\n",
    "            name_to_full_object[base_obj] = obj\n",
    "\n",
    "logging.info(f\"Name mapping created: {name_to_full_object}\")\n",
    "\n",
    "# === Setup results tracking ===\n",
    "results_by_object = defaultdict(lambda: {'correct': 0, 'total': 0})\n",
    "all_correct = 0\n",
    "all_total = 0\n",
    "\n",
    "# Setup save path\n",
    "current_time = datetime.now()\n",
    "formatted_time = current_time.strftime(\"%A_%d_%I_%M_%p\")\n",
    "save_path = os.path.join(args.save_folder, args.task, args.vlm_model, formatted_time)\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "logging.info(f\"Results will be saved to: {save_path}\")\n",
    "\n",
    "# === Main evaluation loop ===\n",
    "logging.info(\"\\n\" + \"=\" * 80)\n",
    "logging.info(\"Starting VQA evaluation\")\n",
    "logging.info(\"=\" * 80)\n",
    "\n",
    "for batch_idx, batch in enumerate(dataloader):\n",
    "    if batch is None:\n",
    "        continue\n",
    "    \n",
    "    # Unpack batch (batch size is 1)\n",
    "    # Returns: img, img_dino, path, label, question, answer, None, label_map, open_question, full_answer\n",
    "    imgs, imgs_dino, paths, labels, questions, answers, _, label_maps, open_questions, full_answers = batch\n",
    "    \n",
    "    # Get single items from batch\n",
    "    img_array = imgs[0]  # Already numpy array\n",
    "    img_dino_array = imgs_dino[0]  # Already numpy array\n",
    "    img_path = paths[0]\n",
    "    obj_name = labels[0]\n",
    "    question = questions[0]\n",
    "    answer = answers[0]\n",
    "    label_map = label_maps[0] if label_maps[0] is not None else {}\n",
    "    \n",
    "    if question is None or answer is None:\n",
    "        logging.warning(f\"Skipping {img_path} - missing QA data\")\n",
    "        continue\n",
    "    \n",
    "    # Parse multi-concept object names\n",
    "    # Multi-concept format: \"Person_Persons object\" (underscore separator)\n",
    "    # Single concept format: \"Object-type\" (hyphen separator, no underscore)\n",
    "    parts = obj_name.split('_', 1)\n",
    "    \n",
    "    if len(parts) == 2:\n",
    "        # Multi-concept case: split into individual objects\n",
    "        object_names = parts  # e.g., [\"Casey\", \"Caseys boosted board\"]\n",
    "    else:\n",
    "        # Single concept case\n",
    "        object_names = [obj_name]  # e.g., [\"Alexs everyday bag-bag\"]\n",
    "    \n",
    "    logging.info(f\"\\n{'=' * 80}\")\n",
    "    logging.info(f\"Processing [{batch_idx + 1}/{len(dataloader)}]\")\n",
    "    logging.info(f\"Original label: {obj_name}\")\n",
    "    logging.info(f\"Objects to detect: {object_names}\")\n",
    "    logging.info(f\"Image: {img_path}\")\n",
    "    \n",
    "    # Get features for ALL objects\n",
    "    obj_features_list = []\n",
    "    valid_objects = []\n",
    "    valid_object_display_names = []  # For display purposes (with label_map applied)\n",
    "    \n",
    "    for obj in object_names:\n",
    "        # Try direct match first\n",
    "        matched_obj = obj\n",
    "        \n",
    "        # If not found, try to map base name to full name\n",
    "        if obj not in object_to_features:\n",
    "            # Try name mapping (e.g., \"Casey\" -> \"Casey-man\")\n",
    "            if obj in name_to_full_object:\n",
    "                matched_obj = name_to_full_object[obj]\n",
    "                logging.info(f\"  Mapped '{obj}' -> '{matched_obj}'\")\n",
    "            else:\n",
    "                logging.warning(f\"  Object '{obj}' not found in feature list. Skipping this object.\")\n",
    "                continue\n",
    "        \n",
    "        if matched_obj not in object_to_features:\n",
    "            logging.warning(f\"  Object '{matched_obj}' not found in feature list. Skipping this object.\")\n",
    "            continue\n",
    "            \n",
    "        obj_features_list.append(object_to_features[matched_obj])\n",
    "        valid_objects.append(matched_obj)\n",
    "        \n",
    "        # Get display name (apply label_map if available)\n",
    "        display_name = label_map.get(obj, obj) if label_map else obj\n",
    "        valid_object_display_names.append(display_name)\n",
    "    \n",
    "    if len(obj_features_list) == 0:\n",
    "        logging.warning(f\"  No valid objects found. Skipping entire sample.\")\n",
    "        continue\n",
    "    \n",
    "    logging.info(f\"  Found {len(valid_objects)} valid objects: {valid_objects}\")\n",
    "    \n",
    "    # Concatenate all object features\n",
    "    obj_features = torch.cat(obj_features_list, dim=0)\n",
    "    \n",
    "    # Convert numpy back to PIL for processing\n",
    "    img_raw = Image.fromarray(img_array.astype(np.uint8))\n",
    "    \n",
    "    # Prepare image batches for DINO and SAM\n",
    "    dino_image_batch = [img_dino_array]\n",
    "    sam_image_batch = [img_dino_array]\n",
    "    \n",
    "    # Get DinoV2 features\n",
    "    dino_img_features = mask_extractor.forward_dino(dino_image_batch)\n",
    "    \n",
    "    # Get SAM masks\n",
    "    sam_masks = mask_extractor.forward_grounding_dino(sam_image_batch, ['object.'])\n",
    "    \n",
    "    # Get the mean Dino features for each mask\n",
    "    category_features = apply_mask_dino(args, sam_masks, dino_img_features[0])\n",
    "    category_features = torch.stack([x.mean(dim=0) for x in category_features])\n",
    "    \n",
    "    # Find best matching masks for ALL objects\n",
    "    bboxes = []\n",
    "    bbox_img = copy.deepcopy(img_raw)\n",
    "    \n",
    "    for idx, obj in enumerate(valid_objects):\n",
    "        obj_feat = obj_features_list[idx]\n",
    "        final_mask, max_score = get_mask_for_gt(args, category_features, sam_masks[0], obj_feat)\n",
    "        \n",
    "        bbox, bbox_raw = mask_to_normalized_bbox(final_mask)\n",
    "        bboxes.append(bbox)\n",
    "        \n",
    "        # Draw bounding box with different color for each object\n",
    "        bbox_img = draw_bounding_box(bbox_img, [bbox], [colors[idx]])\n",
    "        logging.info(f\"  Detected '{valid_objects[idx]}' with score {max_score:.3f}\")\n",
    "    \n",
    "    # Generate prompt with ALL detected objects\n",
    "    # Use display names for the prompt\n",
    "    prompts = get_query(args, valid_object_display_names, colors[:len(valid_objects)], question, None, None)\n",
    "    logging.info(f\"Question: {prompts}\")\n",
    "\n",
    "    \n",
    "    # Save bbox image to temporary file\n",
    "    with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as tmp_file:\n",
    "        tmp_path = tmp_file.name\n",
    "        bbox_img.save(tmp_path)\n",
    "    \n",
    "    try:\n",
    "        #import pdb;pdb.set_trace()\n",
    "        response = vlm_model(tmp_path, [prompts])\n",
    "    finally:\n",
    "        # Clean up temporary file\n",
    "        if os.path.exists(tmp_path):\n",
    "            os.remove(tmp_path)\n",
    "    \n",
    "    # Parse answer - check if multiple choice or open-ended\n",
    "    if 'A.' in question or 'B.' in question:\n",
    "        # Multiple choice - extract letter\n",
    "        predicted_answer = None\n",
    "        response_lower = response.lower().strip()\n",
    "        \n",
    "        # Try to extract answer letter\n",
    "        if response_lower.startswith('a'):\n",
    "            predicted_answer = 'A'\n",
    "        elif response_lower.startswith('b'):\n",
    "            predicted_answer = 'B'\n",
    "        elif 'a.' in response_lower or 'a)' in response_lower:\n",
    "            predicted_answer = 'A'\n",
    "        elif 'b.' in response_lower or 'b)' in response_lower:\n",
    "            predicted_answer = 'B'\n",
    "        \n",
    "        # Extract correct answer letter from answer string (e.g., \"A.option text\")\n",
    "        correct_letter = answer[0] if answer and len(answer) > 0 else None\n",
    "        is_correct = (predicted_answer == correct_letter)\n",
    "    else:\n",
    "        # Open-ended - simple string matching\n",
    "        llm_answer = azure.evaluate_answer(question, response, answer)\n",
    "        #print('LLm Answer:',llm_answer)\n",
    "        if 'yes' in llm_answer.lower():\n",
    "            is_correct = True\n",
    "        elif 'no' in llm_answer.lower():\n",
    "            is_correct = False\n",
    "    \n",
    "    correctness = \"✓ CORRECT\" if is_correct else \"✗ INCORRECT\"\n",
    "    logging.info(f\"Ground Truth: {answer}\")\n",
    "    logging.info(f\"Prediction: {response}\")\n",
    "    logging.info(f\"Result: {correctness}\")\n",
    "    \n",
    "    # Update counters (use original obj_name for tracking)\n",
    "    results_by_object[obj_name]['correct'] += int(is_correct)\n",
    "    results_by_object[obj_name]['total'] += 1\n",
    "    all_correct += int(is_correct)\n",
    "    all_total += 1\n",
    "    \n",
    "    # Save detailed results\n",
    "    with open(os.path.join(save_path, 'results_vqa.txt'), 'a') as file:\n",
    "        file.write('#' * 80 + '\\n')\n",
    "        file.write(f'IMG Path: {img_path}\\n')\n",
    "        file.write(f'Objects: {obj_name} -> {valid_objects}\\n')\n",
    "        file.write(f'Question: {question}\\n')\n",
    "        file.write(f'Ground Truth: {answer}\\n')\n",
    "        file.write(f'Prediction: {response}\\n')\n",
    "        file.write(f'Result: {correctness}\\n')\n",
    "    \n",
    "    if not is_correct and hasattr(args, 'show') and args.show:\n",
    "        plt.imshow(bbox_img)\n",
    "        plt.title(f\"Q: {question}\\nGT: {answer}\\nPred: {response}\")\n",
    "        plt.show()\n",
    "\n",
    "# === Aggregate results ===\n",
    "logging.info(\"\\n\" + \"=\" * 80)\n",
    "logging.info(\"PER-OBJECT RESULTS\")\n",
    "logging.info(\"=\" * 80)\n",
    "\n",
    "per_object = {}\n",
    "for obj, res in results_by_object.items():\n",
    "    acc = res['correct'] / res['total'] * 100 if res['total'] > 0 else 0\n",
    "    logging.info(f\"{obj:40s}: {res['correct']:3d}/{res['total']:3d} = {acc:6.2f}%\")\n",
    "    per_object[obj] = {'correct': res['correct'], 'total': res['total'], 'accuracy': acc}\n",
    "\n",
    "overall_acc = all_correct / all_total * 100 if all_total > 0 else 0\n",
    "logging.info(f\"\\nOVERALL ACCURACY: {all_correct}/{all_total} = {overall_acc:.2f}%\")\n",
    "\n",
    "# === Save results ===\n",
    "results_dict = {\n",
    "    'per_object': per_object,\n",
    "    'overall': {'correct': all_correct, 'total': all_total, 'accuracy': overall_acc}\n",
    "}\n",
    "\n",
    "results_json_path = os.path.join(save_path, 'results_summary.json')\n",
    "with open(results_json_path, 'w') as f:\n",
    "    json.dump(results_dict, f, indent=2)\n",
    "\n",
    "logging.info(f\"\\nResults saved to {results_json_path}\")\n",
    "\n",
    "# Also save in the original format for compatibility\n",
    "with open(os.path.join(save_path, 'results_vqa.txt'), 'a') as file:\n",
    "    file.write('\\n' + '=' * 80 + '\\n')\n",
    "    file.write('SUMMARY\\n')\n",
    "    file.write('=' * 80 + '\\n')\n",
    "    file.write(f'Total Images: {all_total}\\n')\n",
    "    file.write(f'Correct Images: {all_correct}\\n')\n",
    "    file.write(f'Accuracy: {overall_acc:.2f}%\\n')\n",
    "\n",
    "logging.info(\"Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zeroshot_github",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

ğŸ“„ Paper Link: https://arxiv.org/abs/2502.02452

# ğŸ“Œ Personalization Toolkit (PeKit)

## ğŸ“‘ Sections
- ğŸ“ ğŸ“– Overview  
- ğŸ§  Method Summary  
- ğŸ“‚ Dataset: This-is-My-Img  
- âš™ï¸ Installation  
- ğŸš€ Evaluation

- ğŸš€ TODOs  
- ğŸ“š Citation  
- ğŸ“Œ Contribution Guidelines  

---

## ğŸ“– Overview

**Personalization Toolkit (PeKit)** is a *training-free* approach for personalization of Large Vision-Language Models (LVLMs).  
Instead of fine-tuning or test-time training for each new concept, it uses:

- Pre-trained vision foundation models to extract distinctive features  
- Retrieval-Augmented Generation (RAG) to identify instances in visual inputs  
- Visual prompting to guide LVLM outputs efficiently  

This toolkit is model-agnostic, supports **multi-concept personalization**, and works on both **images and videos**.

## ğŸ§  Method Summary

### âœ… Key Components

1. **Training-Free View Extraction**  
   Extract object-level embeddings from reference images using pre-trained vision models (e.g., DINOv2, SAM).

2. **Personalized Object Retrieval**  
   Use a retrieval module over stored features to detect personalized concepts in query images.

3. **Personalized Answer Generation**  
   Generate tailored responses via LVLMs by overlaying visual prompts highlighting detected objects.

---

## ğŸ“‚ Dataset: This-is-My-Img

Google Drive Link: **[ADD LINK HERE]**

### Structure

```text
This-is-My-Img/
â”œâ”€â”€ Single-concept/
â”‚   â”œâ”€â”€ Reference Images/
â”‚   â”œâ”€â”€ Validation/
â”‚       â”œâ”€â”€ positive/
â”‚       â”œâ”€â”€ Negative (Hard)/
â”‚       â”œâ”€â”€ Negative (Other)/
â”‚       â”œâ”€â”€ Fake/
â”œâ”€â”€ Multi-concept/
â”‚   â”œâ”€â”€ Reference Images/
â”‚   â”œâ”€â”€ Validation/
```

---

## âš™ï¸ Installation

Clone the repository and install dependencies:

```bash
git clone https://github.com/your-repo/pekit.git
cd pekit
pip install -r requirements.txt
```

(Optional) Install as a package:

```bash
pip install -e .
```

---

## ğŸš€ Evaluation

### Dataset Folder Structure

Download the datasets and organize them with the following structure:
```
myvlm/
â””â”€â”€ data/
    â””â”€â”€ [29 concepts]/

YoLLaVA/
â”œâ”€â”€ train/
â”‚   â””â”€â”€ [40 concepts]/
â””â”€â”€ test/
    â””â”€â”€ [40 concepts]/

This-is-My-Img/
â”œâ”€â”€ Single-concept/
â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â””â”€â”€ [14 concepts]/
â”‚   â”œâ”€â”€ test/
â”‚   â”‚   â””â”€â”€ [14 concepts]/
â”‚   â””â”€â”€ this-is-my-visual-qa-ambiguity.json
â”‚
â””â”€â”€ Multi-concept/
    â”œâ”€â”€ train/
    â”‚   â””â”€â”€ [21 concepts]/
    â”œâ”€â”€ test/
    â”‚   â””â”€â”€ [11 concept pairs]/
    â””â”€â”€ VQA/
        â””â”€â”€ [VQA files for each multi-concept pair]
```
    


## ğŸš€ TODOs
### ğŸ§‘â€ğŸ’» Code Release

#### ğŸ”¹ Core Pipeline

- [ ] View Extraction  
  - [ ] Implement module for open-vocabulary segmentation (SAM / GroundedDINO)  
  - [ ] Extract patch-level features  
  - [ ] Save embedding vectors in memory  

- [ ] Retrieval System  
  - [ ] Thresholding logic & object matching  
  - [ ] Multiple concept detection  

- [ ] Prompting Integration  
  - [ ] LVLM input formatting  
  - [ ] Prompt templates for VQA & captioning  
  - [ ] Overlay generation for visual cues  
---

### ğŸ§ª Evaluation & Benchmarks

- [ ] Scripts  
  - [ ] Eval suite for VQA  
  - [ ] Eval suite for captioning  
  - [ ] Ablation tests  

- [ ] Metrics  
  - [ ] Accuracy/Precision/recall for personalized retrieval  
  - [ ] Accuracy for Multiple-choice VQA
  - [ ] Accuracy for Open-ended VQA
  - [ ] Captioning Recall

---

## ğŸ“š Citation

```bibtex
@article{seifi2025personalization,
  title={Personalization Toolkit: Training Free Personalization of Large Vision Language Models},
  author={Seifi, Soroush and Dorovatas, Vaggelis and Olmeda Reino, Daniel and Aljundi, Rahaf},
  journal={arXiv preprint arXiv:2502.02452},
  year={2025}
}
```

---
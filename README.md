ğŸ“„ Paper Link: https://arxiv.org/abs/2502.02452

# ğŸ“Œ Personalization Toolkit (PeKit)

## ğŸ“‘ Sections
- ğŸ“ ğŸ“– Overview  
- ğŸ§  Method Summary  
- ğŸ“‚ Dataset: This-is-My-Img  
- âš™ï¸ Installation  
- ğŸš€ Evaluation

- ğŸš€ TODOs  
- ğŸ“š Citation  
- ğŸ“Œ Contribution Guidelines  

---

## ğŸ“– Overview

**Personalization Toolkit (PeKit)** is a *training-free* approach for personalization of Large Vision-Language Models (LVLMs).  
Instead of fine-tuning or test-time training for each new concept, it uses:

- Pre-trained vision foundation models to extract distinctive features  
- Retrieval-Augmented Generation (RAG) to identify instances in visual inputs  
- Visual prompting to guide LVLM outputs efficiently  

This toolkit is model-agnostic, supports **multi-concept personalization**, and works on both **images and videos**.

## ğŸ§  Method Summary

### âœ… Key Components

1. **Training-Free View Extraction**  
   Extract object-level embeddings from reference images using pre-trained vision models (e.g., DINOv2, SAM).

2. **Personalized Object Retrieval**  
   Use a retrieval module over stored features to detect personalized concepts in query images.

3. **Personalized Answer Generation**  
   Generate tailored responses via LVLMs by overlaying visual prompts highlighting detected objects.

---

## ğŸ“‚ Dataset: This-is-My-Img

Google Drive Link: **[ADD LINK HERE]**

### Structure

```text
This-is-My-Img/
â”œâ”€â”€ Single-concept/
â”‚   â”œâ”€â”€ Reference Images/
â”‚   â”œâ”€â”€ Validation/
â”‚       â”œâ”€â”€ positive/
â”‚       â”œâ”€â”€ Negative (Hard)/
â”‚       â”œâ”€â”€ Negative (Other)/
â”‚       â”œâ”€â”€ Fake/
â”œâ”€â”€ Multi-concept/
â”‚   â”œâ”€â”€ Reference Images/
â”‚   â”œâ”€â”€ Validation/
```

---

## âš™ï¸ Installation

Clone the repository and install dependencies:

```bash
git clone https://github.com/your-repo/pekit.git
cd pekit
pip install -r requirements.txt
```

(Optional) Install as a package:

```bash
pip install -e .
```

---

## ğŸš€ Evaluation

### Dataset Folder Structure

Download the datasets and organize them with the following structure:
```
myvlm/
â””â”€â”€ data/
    â””â”€â”€ [29 concepts]/

YoLLaVA/
â”œâ”€â”€ train/
â”‚   â””â”€â”€ [40 concepts]/
â””â”€â”€ test/
    â””â”€â”€ [40 concepts]/

This-is-My-Img/
â”œâ”€â”€ Single-concept/
â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â””â”€â”€ [14 concepts]/
â”‚   â”œâ”€â”€ test/
â”‚   â”‚   â””â”€â”€ [14 concepts]/
â”‚   â””â”€â”€ this-is-my-visual-qa-ambiguity.json
â”‚
â””â”€â”€ Multi-concept/
    â”œâ”€â”€ train/
    â”‚   â””â”€â”€ [21 concepts]/
    â”œâ”€â”€ test/
    â”‚   â””â”€â”€ [11 concept pairs]/
    â””â”€â”€ VQA/
        â””â”€â”€ [VQA files for each multi-concept pair]
```
## Reference View Extraction

Extract reference view features from your dataset using the following command:
```bash
python extraction.py \
  --data_folder ./datasets/ \
  --dataset myvlm \
  --split train \
  --device_ids 0,1,2,3 \
  --n_training_views 5 \
  --variation augment \
  --n_augment 9 \
  --grounding_sam \
  --features_folder ./features/
```

### Arguments

| Argument | Type | Choices | Description |
|----------|------|---------|-------------|
| `--data_folder` | `str` | - | Path to your dataset directory |
| `--dataset` | `str` | `myvlm`, `yollava`, `this-is-my` | Dataset to process |
| `--split` | `str` | `train`, `test` | Data split (must be `train` for reference view extraction) |
| `--variation` | `str` | `normal`, `augment` | Feature extraction mode |
| `--n_augment` | `int` | - | Number of augmented views (only for `variation=augment`) |
| `--grounding_sam` | `flag` | - | Use Grounding SAM for mask extraction (omit to use Grounding DINO) |
| `--multi_concept` | `flag` | - | Process extended concepts (only for `this-is-my` dataset) |
| `--n_training_views` | `int` | - | Number of reference views to extract per concept |
| `--features_folder` | `str` | - | Directory to save extracted feature files |
| `--device_ids` | `str` | - | Comma-separated GPU device IDs (e.g., `0,1,2,3`) |

### Variation Modes

- **`normal`**: Extracts features only from the original reference views
- **`augment`**: Extracts features from both original and augmented reference views

### Example Usage

**Basic extraction with original views only:**
```bash
python extraction.py \
  --data_folder ./datasets/ \
  --dataset yollava \
  --split train \
  --variation normal \
  --n_training_views 3 \
  --features_folder ./features/
```

**Extraction with data augmentation:**
```bash
python extraction.py \
  --data_folder ./datasets/ \
  --dataset this-is-my \
  --split train \
  --variation augment \
  --n_augment 5 \
  --multi_concept \
  --features_folder ./features/
```



## ğŸš€ TODOs
### ğŸ§‘â€ğŸ’» Code Release

#### ğŸ”¹ Core Pipeline

- [ ] View Extraction  
  - [ ] Implement module for open-vocabulary segmentation (SAM / GroundedDINO)  
  - [ ] Extract patch-level features  
  - [ ] Save embedding vectors in memory  

- [ ] Retrieval System  
  - [ ] Thresholding logic & object matching  
  - [ ] Multiple concept detection  

- [ ] Prompting Integration  
  - [ ] LVLM input formatting  
  - [ ] Prompt templates for VQA & captioning  
  - [ ] Overlay generation for visual cues  
---

### ğŸ§ª Evaluation & Benchmarks

- [ ] Scripts  
  - [ ] Eval suite for VQA  
  - [ ] Eval suite for captioning  
  - [ ] Ablation tests  

- [ ] Metrics  
  - [ ] Accuracy/Precision/recall for personalized retrieval  
  - [ ] Accuracy for Multiple-choice VQA
  - [ ] Accuracy for Open-ended VQA
  - [ ] Captioning Recall

---

## ğŸ“š Citation

```bibtex
@article{seifi2025personalization,
  title={Personalization Toolkit: Training Free Personalization of Large Vision Language Models},
  author={Seifi, Soroush and Dorovatas, Vaggelis and Olmeda Reino, Daniel and Aljundi, Rahaf},
  journal={arXiv preprint arXiv:2502.02452},
  year={2025}
}
```

---